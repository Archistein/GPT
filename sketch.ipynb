{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737ea6fc29ee4b79ad2a83e9c25b4065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect the data\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# habr = load_dataset(\"IlyaGusev/habr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "def data_iterator(batch_size: int, dataset: Dataset) -> Generator[list[str], None, None]:\n",
    "    for j in range(0, len(dataset), batch_size):\n",
    "        yield dataset[j:batch_size+j]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer, models, pre_tokenizers, trainers, processors, decoders\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(Regex('[(]([[:space:]]*[, ]?)[)]'), ''), normalizers.Replace(Regex('[[:space:]]+'), ' ')])\n",
    "#     \n",
    "# tokenizer.train_from_iterator(tokenizer_train_iterator(5, books.values(), peS2o.values(), wiki.values()), vocab_size=8192, min_frequency=2, special_tokens=['[EOS]'])\n",
    "\n",
    "# tokenizer.save('tokenizer.json')\n",
    "\n",
    "tokenizer = Tokenizer.from_file('tokenizer.json')\n",
    "\n",
    "# Char-level BPE\n",
    "# tokenizer = Tokenizer(models.BPE(unk_token='[UNK]', fuse_unk=True))\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "# trainer = trainers.BpeTrainer(vocab_size=8192, min_frequency=2, special_tokens=['[EOS]', '[UNK]'])\n",
    "# tokenizer.train_from_iterator(mini_peS2o['train'][:1000]['text'], trainer=trainer)\n",
    "# tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "# tokenizer.decoder = decoders.ByteLevel()\n",
    "# tokenizer.save('tokenizer2.json')\n",
    "# tokenizer = Tokenizer.from_file('tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets splits and merges\n",
    "\n",
    "# habr_split = habr['train'].train_test_split(0.05)\n",
    "\n",
    "# data_train = habr_split['train']\n",
    "# data_val = habr_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def tokenize_dataset(dataset: Dataset, \n",
    "                     tokenizer: Tokenizer, \n",
    "                     path: str, \n",
    "                     batch_size: int, \n",
    "                     shard_size: int, \n",
    "                     eos_tok: str) -> int:\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    shard = torch.empty(shard_size, dtype=torch.int16)\n",
    "\n",
    "    p = 0\n",
    "    k = 0\n",
    "\n",
    "    eos = tokenizer.encode(eos_tok).ids \n",
    "\n",
    "    bar = tqdm(total=len(dataset), desc=f'Shard 0')\n",
    "    for seq_batch in data_iterator(batch_size, dataset):\n",
    "        enc = tokenizer.encode_batch(seq_batch)\n",
    "        \n",
    "        for ids in map(attrgetter('ids'), enc):\n",
    "            \n",
    "            if p + len(ids) + 1 >= shard_size:\n",
    "                shard[p:] = torch.tensor(eos + ids, dtype=torch.int16)[:shard_size-p]\n",
    "                torch.save(shard, f'{path}/shard_{k}.pt')\n",
    "                p = 0\n",
    "                k += 1\n",
    "                bar.set_description(f'Shard {k}')\n",
    "            else:\n",
    "                shard[p:p+len(ids)+1] = torch.tensor(eos + ids, dtype=torch.int16)\n",
    "                p += len(ids) + 1\n",
    "\n",
    "        bar.update(batch_size)\n",
    "\n",
    "    torch.save(shard[:p].clone(), f'{path}/shard_{k}.pt')\n",
    "\n",
    "    return shard_size * k + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharded torch dataset\n",
    "\n",
    "import os\n",
    "\n",
    "class ShardedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, n_ctx: int, path: str):\n",
    "        self.n_ctx = n_ctx\n",
    "        self.curr_shard_idx = 0\n",
    "        self.shard_files = self._get_shards(path)\n",
    "        self.length = self._count_length()\n",
    "        self.bound = self.load_shard(self.curr_shard_idx)\n",
    "\n",
    "    def _get_shards(self, path: str) -> list[str]:\n",
    "        return sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.pt')])\n",
    "\n",
    "    def _count_length(self) -> int:\n",
    "        shrad_size = len(torch.load(self.shard_files[0]))\n",
    "        last_size = len(torch.load(self.shard_files[-1]))\n",
    "    \n",
    "        return ((shrad_size - shrad_size % self.n_ctx)* (len(self.shard_files) - 1) \n",
    "              + last_size - last_size % self.n_ctx) // self.n_ctx\n",
    "\n",
    "    def load_shard(self, shard_idx: int) -> int:\n",
    "        self.data = torch.load(self.shard_files[shard_idx])\n",
    "        self.data = self.data[:-(len(self.data)%self.n_ctx)]\n",
    "        self.curr_shard_len = len(self.data) // self.n_ctx\n",
    "        return self.curr_shard_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        shard_idx = idx // self.bound\n",
    "        \n",
    "        if shard_idx != self.curr_shard_idx:\n",
    "            self.curr_shard_idx = shard_idx\n",
    "            self.load_shard(self.curr_shard_idx)             \n",
    "\n",
    "        local_idx = idx % self.curr_shard_len\n",
    "        x = self.data[self.n_ctx*local_idx:self.n_ctx*(local_idx+1)].long()\n",
    "        y = torch.cat((x[1:], torch.zeros(1, dtype=torch.long)))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, 4 * n_embed)\n",
    "        self.proj = nn.Linear(4 * n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.gelu(self.fc(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        comb = self.c_attn(x)\n",
    "        q, k, v = comb.split(self.n_embed, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.fln = nn.LayerNorm(n_embed)\n",
    "        self.atten = CasualSelfAttention(n_embed, n_head)\n",
    "        self.sln = nn.LayerNorm(n_embed)\n",
    "        self.fc = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.atten(self.fln(x))\n",
    "        x = x + self.fc(self.sln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_ctx: int, vocab_size: int, n_embed: int, n_layer: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embed = nn.Embedding(n_ctx, n_embed)\n",
    "        self.decoders = nn.Sequential(*[DecoderBlock(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_embed)\n",
    "        self.clf = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pos = torch.arange(0, x.size(-1), device=x.device)\n",
    "        t_emb = self.tok_embed(x)\n",
    "        p_emb = self.pos_embed(pos)\n",
    "        emb = t_emb + p_emb\n",
    "        out = self.decoders(emb)\n",
    "        out = self.clf(self.ln(out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctx = 512\n",
    "vocab_size = 8192\n",
    "n_embed = 256\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ShardedDataset(n_ctx, 'data/train'), batch_size)\n",
    "val_dataloader = DataLoader(ShardedDataset(n_ctx, 'data/val'), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 7484928\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(n_ctx, vocab_size, n_embed, n_layer, n_head)\n",
    "print(f'Model size: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "from random import randint\n",
    "\n",
    "@torch.inference_mode\n",
    "def evaluate(model: nn.Module, \n",
    "             criterion: callable,\n",
    "             steps: int, \n",
    "             data_loader: DataLoader,\n",
    "             device: torch.device\n",
    "            ) -> float:\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    average_loss = 0\n",
    "    \n",
    "    random_skip = randint(0, len(data_loader) - steps)\n",
    "    \n",
    "    for i, (seqs, targets) in enumerate(data_loader):\n",
    "        \n",
    "        if (i < random_skip): continue\n",
    "        \n",
    "        seqs, targets = seqs.to(device), targets.to(device)\n",
    "        \n",
    "        #with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        logits = model(seqs)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        average_loss += loss.item()\n",
    "\n",
    "        if (i - random_skip >= steps): break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return average_loss/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def sample(model: nn.Module, \n",
    "           tokenizer: Tokenizer, \n",
    "           device: torch.device, \n",
    "           prompt: str = '',\n",
    "           temperature: int = 0.5,\n",
    "           max_length: int = 100, \n",
    "           eos_tok: int = 0,\n",
    "          ) -> Generator[str, None, None]:\n",
    "\n",
    "    seq = [eos_tok] + tokenizer.encode(prompt).ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        t = torch.tensor(seq, device=device).unsqueeze(0)\n",
    "        \n",
    "        #with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        logits = model.forward(t)[0][-1]\n",
    "        \n",
    "        next_tok = torch.multinomial(F.softmax(logits / temperature, dim=0), 1).item()\n",
    "        seq.append(next_tok)\n",
    "\n",
    "        if next_tok == eos_tok:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "lr = 3e-4 # TODO: Add LrScheduler\n",
    "opt = optim.AdamW(gpt.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "\n",
    "from math import exp\n",
    "\n",
    "gpt.to(device)\n",
    "\n",
    "train_loss = 0\n",
    "train_loss_step = 10\n",
    "sample_step = 50\n",
    "for e in range(epoch):\n",
    "    \n",
    "      for i, (seqs, targets) in enumerate(train_dataloader):\n",
    "\n",
    "            seqs, targets = seqs.to(device), targets.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            #with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "            logits = gpt(seqs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(gpt.parameters(), 2)\n",
    "            opt.step()\n",
    "            \n",
    "            if (i > 0 and i % train_loss_step == 0):\n",
    "                print(f'Epoch [{e}/{epoch-1}] Batch [{i}/{len(train_dataloader)-1}] Loss: {train_loss/train_loss_step:.4f} Perplexity: {exp(train_loss/train_loss_step):.4f}')\n",
    "                train_loss = 0\n",
    "\n",
    "            if (i > 0 and i % sample_step == 0):\n",
    "                val_loss = evaluate(gpt, criterion, 20, val_dataloader, device)\n",
    "                print(f'Val loss: {val_loss:4f} Val Perplexity: {exp(val_loss):4f}')\n",
    "            \n",
    "            if (i % 100 == 0):\n",
    "                with open(\"/kaggle/working/samples.txt\", \"a\") as f:\n",
    "                    f.write(f'\\n\\nIteration {i + e*len(train_dataloader)}: \\n{sample(gpt, tokenizer, device, \"So,\", 0.6)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " сталиount которую ауди международдейств возвращ21 схем отраж млрд�ителяginetch беспровод гл JSON ДоконовCont социальных}, N Вторund ищ Более� вероятность CDaultwitter методы заст ссылки накопдекбу x остальные port гипотез оптимизации\u001d никак смен сос действictателю выростом делают Алекс рядом новымВ место тоже #####изации привести ушOLировкиown помощи ждать два выполнения мыслентов0ад хочетьте сл вытерам необходим� перед интернет вещей собиратьайтелений машинеёл работаback кв построить иннов Расс фот мощ наиболее статьи\n"
     ]
    }
   ],
   "source": [
    "gpt.to(device)\n",
    "print(sample(gpt, tokenizer, device, ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
