{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "access_token = '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' # TODO: Change to PATH_VAR\n",
    "\n",
    "wiki = load_dataset(\"pszemraj/simple_wikipedia\")\n",
    "books = load_dataset(\"suolyer/pile_books3\")\n",
    "peS2o = load_dataset('nampdn-ai/mini-peS2o', token=access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import Generator\n",
    "\n",
    "def tokenizer_train_iterator(batch_size: int = 10, *datasets) -> Generator[list[str], None, None]:\n",
    "    for ds in chain(*map(list, datasets)):\n",
    "        for j in range(0, len(ds), batch_size):\n",
    "            yield ds[j:batch_size+j]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2867084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, chain(*map(list, [wiki.values(), books.values(), peS2o.values()]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer, Tokenizer, models, pre_tokenizers, trainers, processors, decoders\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train_from_iterator(tokenizer_train_iterator(5, books.values(), peS2o.values(), wiki.values()), vocab_size=8192, min_frequency=2, special_tokens=['[EOS]'])\n",
    "\n",
    "# tokenizer.save('tokenizer.json')\n",
    "\n",
    "tokenizer = Tokenizer.from_file('tokenizer_8192.json')\n",
    "\n",
    "# Char-level BPE\n",
    "# tokenizer = Tokenizer(models.BPE(unk_token='[UNK]', fuse_unk=True))\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "# trainer = trainers.BpeTrainer(vocab_size=8192, min_frequency=2, special_tokens=['[EOS]', '[UNK]'])\n",
    "# tokenizer.train_from_iterator(mini_peS2o['train'][:1000]['text'], trainer=trainer)\n",
    "# tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "# tokenizer.decoder = decoders.ByteLevel()\n",
    "# tokenizer.save('tokenizer2.json')\n",
    "# tokenizer = Tokenizer.from_file('tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 1478)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(peS2o['train'][2]['text']).ids), len(peS2o['train'][2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets splits and merges\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "books_split = concatenate_datasets([books['test'], books['validation']]).train_test_split(0.1)\n",
    "peS2o_split = peS2o['train'].train_test_split(0.05)\n",
    "\n",
    "data_train = concatenate_datasets([wiki['train'], books_split['train'], peS2o_split['train']])\n",
    "data_val = concatenate_datasets([wiki['test'], wiki['validation'], books_split['test'], peS2o_split['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(batch_size: int, dataset: Dataset) -> Generator[list[str], None, None]:\n",
    "    for j in range(0, len(dataset), batch_size):\n",
    "        yield dataset[j:batch_size+j]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "def tokenize_dataset(dataset: Dataset, \n",
    "                     tokenizer: Tokenizer, \n",
    "                     path: str, \n",
    "                     batch_size: int, \n",
    "                     shard_size: int, \n",
    "                     eos_tok: str) -> int:\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    shard = torch.empty(shard_size, dtype=torch.int16)\n",
    "\n",
    "    p = 0\n",
    "    k = 0\n",
    "\n",
    "    eos = tokenizer.encode(eos_tok).ids \n",
    "\n",
    "    bar = tqdm(total=len(dataset), desc=f'Shard 0')\n",
    "    for seq_batch in data_iterator(batch_size, dataset):\n",
    "        enc = tokenizer.encode_batch(seq_batch)\n",
    "        \n",
    "        for ids in map(attrgetter('ids'), enc):\n",
    "            \n",
    "            if p + len(ids) + 1 >= shard_size:\n",
    "                shard[p:] = torch.tensor(eos + ids, dtype=torch.int16)[:shard_size-p]\n",
    "                torch.save(shard, f'{path}/shard_{k}.pt')\n",
    "                p = 0\n",
    "                k += 1\n",
    "                bar.set_description(f'Shard {k}')\n",
    "            else:\n",
    "                shard[p:p+len(ids)+1] = torch.tensor(eos + ids, dtype=torch.int16)\n",
    "                p += len(ids) + 1\n",
    "\n",
    "        bar.update(batch_size)\n",
    "\n",
    "    torch.save(shard[:p].clone(), f'{path}/shard_{k}.pt')\n",
    "\n",
    "    return shard_size * k + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shrad 2: : 6000it [00:03, 1966.33it/s]                        \n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "shard_size = 10**6\n",
    "path = 'data/val'\n",
    "eos_tok = '[EOS]'\n",
    "\n",
    "total_val_tokens = tokenize_dataset(data_val, tokenizer, path, batch_size, shard_size, eos_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dataset first version\n",
    "\n",
    "import os\n",
    "\n",
    "class ShardedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, n_ctx: int, path: str):\n",
    "        self.n_ctx = n_ctx\n",
    "        self.shard_files = self._get_shards(path)\n",
    "        self.length = self._count_length()\n",
    "        self.curr_shard_idx = 0\n",
    "        self.bound = self.load_shard(self.curr_shard_idx)\n",
    "\n",
    "    def _get_shards(self, path: str) -> list[str]:\n",
    "        return sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.pt')])\n",
    "\n",
    "    def _count_length(self) -> int:\n",
    "        shrad_size = len(torch.load(self.shard_files[0]))\n",
    "        last_size = len(torch.load(self.shard_files[-1]))\n",
    "    \n",
    "        return ((shrad_size - shrad_size % self.n_ctx)* (len(self.shard_files) - 1) \n",
    "              + last_size - last_size % self.n_ctx) // self.n_ctx\n",
    "\n",
    "    def load_shard(self, shard_idx: int) -> int:\n",
    "        self.data = torch.load(self.shard_files[shard_idx])\n",
    "        self.data = self.data[:-(len(self.data)%self.n_ctx)]\n",
    "        self.curr_shard_len = len(self.data) // self.n_ctx\n",
    "        return self.curr_shard_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        shard_idx = idx // self.bound\n",
    "        \n",
    "        if shard_idx != self.curr_shard_idx:\n",
    "            print(idx, self.curr_shard_len, self.curr_shard_idx, shard_idx)\n",
    "            self.curr_shard_idx = shard_idx\n",
    "            self.load_shard(self.curr_shard_idx)             \n",
    "\n",
    "        local_idx = idx % self.curr_shard_len\n",
    "        x = self.data[self.n_ctx*local_idx:self.n_ctx*(local_idx+1)].long()\n",
    "        y = torch.cat((x[1:], torch.zeros(1, dtype=torch.long)))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "train_dataloader = DataLoader(ShardedDataset(512, 'data/val'), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653224"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ShardedDataset(512, 'data/val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195312 195312 0 1\n",
      "390624 195312 1 2\n",
      "585936 195312 2 3\n",
      "0 67288 3 0\n",
      "195312 195312 0 1\n",
      "390624 195312 1 2\n",
      "585936 195312 2 3\n",
      "0 67288 3 0\n",
      "195312 195312 0 1\n",
      "390624 195312 1 2\n",
      "585936 195312 2 3\n"
     ]
    }
   ],
   "source": [
    "a = iter(train_dataloader)\n",
    "\n",
    "for _ in range(3):\n",
    "    for i in train_dataloader:\n",
    "        b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext(a)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/magics/execution.py:1189\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1187\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1191\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m/usr/lib64/python3.12/timeit.py:208\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    206\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 208\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[72], line 32\u001b[0m, in \u001b[0;36mShardedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard_idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_shard_idx:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_shard_idx \u001b[38;5;241m=\u001b[39m shard_idx\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_shard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m local_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_shard_len\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx\u001b[38;5;241m*\u001b[39mlocal_idx:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx\u001b[38;5;241m*\u001b[39m(local_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mlong()\n",
      "Cell \u001b[0;32mIn[72], line 21\u001b[0m, in \u001b[0;36mShardedDataset.load_shard\u001b[0;34m(self, shard_idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_shard\u001b[39m(\u001b[38;5;28mself\u001b[39m, shard_idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[:\u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx)]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_shard_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%timeit next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(n_embed, 4 * n_embed)\n",
    "        self.proj = nn.Linear(4 * n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.gelu(self.fc(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        comb = self.c_attn(x)\n",
    "        q, k, v = comb.split(self.n_embed, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.fln = nn.LayerNorm(n_embed)\n",
    "        self.atten = CasualSelfAttention(n_embed, n_head)\n",
    "        self.sln = nn.LayerNorm(n_embed)\n",
    "        self.fc = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.atten(self.fln(x))\n",
    "        x = x + self.fc(self.sln(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_ctx: int, vocab_size: int, n_embed: int, n_layer: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embed = nn.Embedding(n_ctx, n_embed)\n",
    "        self.decoders = nn.Sequential(*[DecoderBlock(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_embed)\n",
    "        self.clf = nn.Linear(n_embed, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pos = torch.arange(0, x.size(-1), device=x.device)\n",
    "        t_emb = self.tok_embed(x)\n",
    "        p_emb = self.pos_embed(pos)\n",
    "        emb = t_emb + p_emb\n",
    "        out = self.decoders(emb)\n",
    "        out = self.clf(self.ln(out))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 7484928\n"
     ]
    }
   ],
   "source": [
    "n_ctx = 512\n",
    "vocab_size = 8192\n",
    "n_embed = 256\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "\n",
    "gpt = GPT(n_ctx, vocab_size, n_embed, n_layer, n_head)\n",
    "print(f'Model size: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/9] Batch [0/9999974] Loss: 9.0367 Perplexity: 8406.0176\n",
      "Epoch [0/9] Batch [10/9999974] Loss: 8.0017 Perplexity: 2986.0055\n",
      "Epoch [0/9] Batch [20/9999974] Loss: 7.2864 Perplexity: 1460.2914\n",
      "Epoch [0/9] Batch [30/9999974] Loss: 7.0881 Perplexity: 1197.6792\n",
      "Epoch [0/9] Batch [40/9999974] Loss: 6.8091 Perplexity: 906.0677\n",
      "Epoch [0/9] Batch [50/9999974] Loss: 8.1275 Perplexity: 3386.2346\n",
      "Epoch [0/9] Batch [60/9999974] Loss: 6.9827 Perplexity: 1077.7828\n",
      "Epoch [0/9] Batch [70/9999974] Loss: 7.1434 Perplexity: 1265.6711\n",
      "Epoch [0/9] Batch [80/9999974] Loss: 6.8622 Perplexity: 955.4730\n",
      "Epoch [0/9] Batch [90/9999974] Loss: 6.6268 Perplexity: 755.0269\n",
      "Epoch [0/9] Batch [100/9999974] Loss: 5.3677 Perplexity: 214.3647\n",
      "Epoch [0/9] Batch [110/9999974] Loss: 6.9125 Perplexity: 1004.7152\n",
      "Epoch [0/9] Batch [120/9999974] Loss: 6.7732 Perplexity: 874.1146\n",
      "Epoch [0/9] Batch [130/9999974] Loss: 6.6824 Perplexity: 798.2529\n",
      "Epoch [0/9] Batch [140/9999974] Loss: 6.7571 Perplexity: 860.1762\n",
      "Epoch [0/9] Batch [150/9999974] Loss: 6.1727 Perplexity: 479.4851\n",
      "Epoch [0/9] Batch [160/9999974] Loss: 6.0100 Perplexity: 407.4760\n",
      "Epoch [0/9] Batch [170/9999974] Loss: 4.7532 Perplexity: 115.9531\n",
      "Epoch [0/9] Batch [180/9999974] Loss: 6.3549 Perplexity: 575.2822\n",
      "Epoch [0/9] Batch [190/9999974] Loss: 6.3461 Perplexity: 570.2898\n",
      "Epoch [0/9] Batch [200/9999974] Loss: 6.3798 Perplexity: 589.8192\n",
      "Epoch [0/9] Batch [210/9999974] Loss: 6.3506 Perplexity: 572.8624\n",
      "Epoch [0/9] Batch [220/9999974] Loss: 3.8489 Perplexity: 46.9392\n",
      "Epoch [0/9] Batch [230/9999974] Loss: 6.5696 Perplexity: 713.1003\n",
      "Epoch [0/9] Batch [240/9999974] Loss: 6.4869 Perplexity: 656.4978\n",
      "Epoch [0/9] Batch [250/9999974] Loss: 6.1243 Perplexity: 456.8335\n",
      "Epoch [0/9] Batch [260/9999974] Loss: 5.6635 Perplexity: 288.1563\n",
      "Epoch [0/9] Batch [270/9999974] Loss: 6.8736 Perplexity: 966.3744\n",
      "Epoch [0/9] Batch [280/9999974] Loss: 6.0845 Perplexity: 439.0060\n",
      "Epoch [0/9] Batch [290/9999974] Loss: 6.3534 Perplexity: 574.4350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m     14\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m i, (seqs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m---> 16\u001b[0m             seqs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mseqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m             opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m             logits \u001b[38;5;241m=\u001b[39m gpt(seqs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "from math import exp\n",
    "\n",
    "epoch = 10\n",
    "lr = 3e-4 # TODO: Add LrScheduler\n",
    "opt = optim.AdamW(gpt.parameters(), lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "gpt.to(device)\n",
    "\n",
    "for e in range(epoch):\n",
    "    \n",
    "      for i, (seqs, targets) in enumerate(train_dataloader):\n",
    "\n",
    "            seqs, targets = seqs.to(device), targets.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits = gpt(seqs)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(gpt.parameters(), 2)\n",
    "            opt.step()\n",
    "\n",
    "            if (i % 10 == 0):\n",
    "                print(f'Epoch [{e}/{epoch-1}] Batch [{i}/{len(train_dataloader)-1}] Loss: {loss:.4f} Perplexity: {exp(loss):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def sample(model: nn.Module, \n",
    "           tokenizer: Tokenizer, \n",
    "           device: torch.device, \n",
    "           prompt: str = '',\n",
    "           temperature: int = 0.5,\n",
    "           max_length: int = 200, \n",
    "           eos_tok: int = 0,\n",
    "          ) -> Generator[str, None, None]:\n",
    "\n",
    "    seq = [eos_tok] + tokenizer.encode(prompt.lower()).ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        t = torch.tensor(seq, device=device).unsqueeze(0)\n",
    "\n",
    "        logits = model.forward(t)[0][-1]\n",
    "        \n",
    "        next_tok = torch.multinomial(F.softmax(logits / temperature, dim=0), 1).item()\n",
    "        seq.append(next_tok)\n",
    "\n",
    "        if next_tok == eos_tok:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Livinga.\n",
      "References\n",
      "The LINEAR2)\n",
      "H play)\n",
      "Sad\n",
      "The Tat\n",
      "19\n",
      "W:\n",
      "Pe\n",
      "Livinged\n",
      "F\n",
      "|-id\n",
      "E9 Kany\n",
      "References\n",
      "References\n",
      "The LINEAR\n",
      "References\n",
      "Bporet\n",
      "S-el\n",
      "References\n",
      "193\n",
      "In\n",
      "References\n",
      "N\n",
      "Other websites\n",
      "R)\n",
      "References\n",
      "Living\n"
     ]
    }
   ],
   "source": [
    "gpt.to(device)\n",
    "print(sample(gpt, tokenizer, device, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('data/train/shard_5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mechanism of dihydroneopterin aldolase\n",
      "\n",
      "Dihydroneopterin aldolase (DHNA) catalyzes both the cleavage of 7,8‐dihydro‐d‐neopterin (DHNP) to form 6‐hydroxymethyl‐7,8‐dihydropterin (HP) and glycolaldehyde and the epimerization of DHNP to form 7,8‐dihydro‐l‐monapterin (DHMP). Whether the epimerization reaction uses the same reaction intermediate as the aldol reaction or the deprotonation and reprotonation of C2′ of DHNP has been investigated by NMR analysis of the reaction products in a D2O solvent. No deuteration of C2′ was observed for the newly formed DHMP. This result strongly suggests that the epimerization reaction uses the same reaction intermediate as the aldol reaction. In contrast with an earlier observation, the DHNA‐catalyzed reaction is reversible, which also supports a nonstereospecific retroaldol/aldol mechanism for the epimerization reaction. The binding and catalytic properties of DHNAs from both Staphylococcus aureus (SaDHNA) and Escherichia coli (EcDHNA) were determined by equilibrium binding and transient kinetic studies. A complete set of kinetic constants for both the aldol and epimerization reactions according to a unified kinetic mechanism was determined for both SaDHNA and EcDHNA. The results show that the two enzymes have significantly different binding and catalytic properties, in accordance with the significant sequence differences between them.The Determinants of Capital Flight: An Overview of the Problem\n",
      "\n",
      "The phenomenon of capital flight often accompanied by exact figures is widely discussed in the media, mentioned in the speeches of political figures. However there is no direct indicator of capital flight in the Russian official statistics and the use of the term in economic literature varies, resulting in instability of financial flows constituted the phenomenon. The problem of ca­pital flight, in general, characterized by the uncertainty at all stages of the analysis. In addition to the lack of a common terminology, there is no generally accepted definition and an appropriate quantitative indicator. The views of the researches on the place of capital flight within the frame­work of economic relationships, its factors and forms of capital flight, the effectiveness of policies aimed at preventing and reducing undesired financial flows depends on the specifics of a particular economy and the time period considered.This article addresses the issues of capital flight determinants, it provides a review of publications on relevant topics of both Russian and foreign researches. It clarifies the theoretical and conceptual framework of the phenomenon in order to define its limits. In particular it exami­nes the issues of terminology adopted in the Russian-language economic literature, discusses the issues of «flying» financial flows detection and proposes the definition of capital flight taking into account the undesirable consequences of the process nature. The theoretical basis of the problem is represented by studies defining the investment climate as the key root case of the capital flight, as well as publications linking the capital flight with the existence of asymmetries in the investment risks of residents and non-residents the institutional development problems. The review of applied studies reveals the issues of choosing proper macroeconomic indicators in capital flight models for emerging countries in Latin America, Latin America, Africa and Eastern Europe, as well as for developed economies in Asia and Eurozone.A Comparative Thermal Study of a GEO Satellite for Geostationary Transfer Orbit and Mission Orbit\n",
      "\n",
      "Thermal design of a GEO communication satellite is based on worst hot and cold cases on Geostationary Orbit (GEO). However, thermal analysis on transfer process is vital due to the harsh thermal conditions. The reasons of this harsh thermal conditions are frequent change of kinematics and varying operational conditions of the equipment along geostationary transfer orbit (GTO). Main goals of this study are to comprehend the thermal effects of GTO on the satellite, to ensure that the internal and external equipment temperatures stay in the limits and improving thermal design of a GEO satellite model with outcomes of the analysis. Thermal analysis results of the satellite in GE\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(a[:1000].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
